{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Лабораторная работа 6: Логистическая регрессия\n",
                "\n",
                "## Реализация без использования библиотек машинного обучения, только NumPy и Pandas"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    f1_score,\n",
                "    confusion_matrix,\n",
                ")\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "\n",
                "# Настройка отображения графиков\n",
                "%matplotlib inline\n",
                "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
                "sns.set_style(\"whitegrid\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Загрузка и предварительная обработка данных"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Загрузка данных\n",
                "df = pd.read_csv(\"data/Titanic.csv\")\n",
                "\n",
                "print(\"1. ПРЕДВАРИТЕЛЬНАЯ ОБРАБОТКА ДАННЫХ\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Проверка на отсутствующие значения\n",
                "print(\"Проверка отсутствующих значений:\")\n",
                "print(df.isnull().sum())\n",
                "print()\n",
                "\n",
                "# Информация о датасете\n",
                "print(\"Информация о датасете:\")\n",
                "print(df.info())\n",
                "print()\n",
                "\n",
                "# Выделение признаков и целевой переменной\n",
                "X = df.drop(\"Survived\", axis=1)\n",
                "y = df[\"Survived\"]\n",
                "\n",
                "print(f\"Размерность признаков: {X.shape}\")\n",
                "print(f\"Размерность целевой переменной: {y.shape}\")\n",
                "print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Статистика и визуализация данных"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"2. СТАТИСТИКА И ВИЗУАЛИЗАЦИЯ ДАННЫХ\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Базовая статистика\n",
                "print(\"Статистика датасета:\")\n",
                "print(df.describe())\n",
                "print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Визуализация распределения классов\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.countplot(x=\"Survived\", data=df)\n",
                "plt.title(\"Распределение выживших пассажиров\")\n",
                "plt.xlabel(\"Выжил (1 - Да, 0 - Нет)\")\n",
                "plt.ylabel(\"Количество\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1. Визуализация распределения признаков"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"2.1. ВИЗУАЛИЗАЦИЯ РАСПРЕДЕЛЕНИЯ ПРИЗНАКОВ\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Создание сетки графиков\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "fig.suptitle(\"Распределение признаков датасета Титаника\", fontsize=16, fontweight=\"bold\")\n",
                "\n",
                "# Построение гистограмм для каждого признака\n",
                "features = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
                "for i, feature in enumerate(features):\n",
                "    row = i // 3\n",
                "    col = i % 3\n",
                "\n",
                "    axes[row, col].hist(df[feature], bins=15, color=\"skyblue\", edgecolor=\"black\", alpha=0.7)\n",
                "    axes[row, col].set_title(f\"Распределение {feature}\", fontweight=\"bold\")\n",
                "    axes[row, col].set_xlabel(feature)\n",
                "    axes[row, col].set_ylabel(\"Частота\")\n",
                "    axes[row, col].grid(True, alpha=0.3)\n",
                "\n",
                "    mean_val = df[feature].mean()\n",
                "    axes[row, col].axvline(\n",
                "        mean_val,\n",
                "        color=\"red\",\n",
                "        linestyle=\"--\",\n",
                "        linewidth=2,\n",
                "        label=f\"Среднее: {mean_val:.2f}\",\n",
                "    )\n",
                "    axes[row, col].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Разделение данных и предобработка"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"3. РАЗДЕЛЕНИЕ ДАННЫХ И ПРЕДОБРАБОТКА\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "\n",
                "# Обработка категориальных признаков\n",
                "def preprocess_data(df):\n",
                "    df_processed = df.copy()\n",
                "\n",
                "    # Кодируем пол (Sex)\n",
                "    df_processed[\"Sex\"] = df_processed[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
                "\n",
                "    # Кодируем порт посадки (Embarked)\n",
                "    embarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
                "    df_processed[\"Embarked\"] = df_processed[\"Embarked\"].map(embarked_mapping)\n",
                "\n",
                "    # Заполняем пропущенные значения\n",
                "    df_processed[\"Age\"] = df_processed[\"Age\"].fillna(df_processed[\"Age\"].median())\n",
                "    df_processed[\"Embarked\"] = df_processed[\"Embarked\"].fillna(0)\n",
                "    df_processed[\"Fare\"] = df_processed[\"Fare\"].fillna(df_processed[\"Fare\"].median())\n",
                "\n",
                "    # Удаляем столбцы, которые сложно кодировать\n",
                "    columns_to_drop = [\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\"]\n",
                "    df_processed = df_processed.drop(\n",
                "        columns=[col for col in columns_to_drop if col in df_processed.columns]\n",
                "    )\n",
                "\n",
                "    return df_processed\n",
                "\n",
                "\n",
                "# Предобработка данных\n",
                "X_processed = preprocess_data(X)\n",
                "\n",
                "print(\"Признаки после предобработки:\")\n",
                "print(X_processed.columns.tolist())\n",
                "print()\n",
                "\n",
                "# Масштабирование признаков (стандартизация)\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X_processed)\n",
                "\n",
                "# Разделение на обучающую и тестовую выборки\n",
                "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
                "\n",
                "print(f\"Обучающая выборка: {X_train_scaled.shape}\")\n",
                "print(f\"Тестовая выборка: {X_test_scaled.shape}\")\n",
                "print(f\"Доля выживших в обучающей выборке: {y_train.mean():.3f}\")\n",
                "print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Реализация логистической регрессии"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LogisticRegression:\n",
                "    def __init__(self):\n",
                "        self.weights = None\n",
                "        self.bias = None\n",
                "        self.losses = []\n",
                "\n",
                "    def sigmoid(self, t):\n",
                "        # Добавляем ограничение для численной стабильности\n",
                "        t = np.clip(t, -500, 500)  # Предотвращаем переполнение\n",
                "        return 1 / (1 + np.exp(-t))\n",
                "\n",
                "    def log_loss(self, y_actual, y_predicted):\n",
                "        # Добавляем эпсилон для избежания log(0)\n",
                "        epsilon = 1e-15\n",
                "        y_predicted = np.clip(y_predicted, epsilon, 1 - epsilon)\n",
                "        return -np.mean(y_actual * np.log(y_predicted) + (1 - y_actual) * np.log(1 - y_predicted))\n",
                "\n",
                "    def gradient_descent(self, X_train, y_train, iterations=1000, learning_rate=0.01):\n",
                "        objects_num, characteristics_num = X_train.shape\n",
                "        self.weights = np.random.normal(0, 0.01, characteristics_num)\n",
                "        self.bias = 0\n",
                "        self.losses = []\n",
                "\n",
                "        for iteration in range(1, iterations + 1):\n",
                "            t = np.dot(X_train, self.weights) + self.bias\n",
                "            z = self.sigmoid(t)\n",
                "\n",
                "            # Можно вычислить dz/dw и dLoss/dz -> dLoss/dw = (1/n) * X^T * (Z - Y)\n",
                "            dw = (1 / objects_num) * np.dot(X_train.T, (z - y_train))\n",
                "            # Аналогично вычисляется dLoss/db\n",
                "            db = (1 / objects_num) * np.sum(z - y_train)\n",
                "\n",
                "            # w = w - α * ∇J(w)\n",
                "            self.weights -= learning_rate * dw\n",
                "            self.bias -= learning_rate * db\n",
                "\n",
                "            if iteration % 100 == 0:\n",
                "                loss = self.log_loss(y_train, z)\n",
                "                self.losses.append(loss)\n",
                "\n",
                "    def newton_optimization(self, X_train, y_train, iterations=1000):\n",
                "        objects_num, characteristics_num = X_train.shape\n",
                "        self.weights = np.random.normal(0, 0.01, characteristics_num)\n",
                "        self.bias = 0\n",
                "        self.losses = []\n",
                "\n",
                "        for iteration in range(1, iterations + 1):\n",
                "            t = np.dot(X_train, self.weights) + self.bias\n",
                "            z = self.sigmoid(t)\n",
                "\n",
                "            dw = (1 / objects_num) * np.dot(X_train.T, (z - y_train))\n",
                "            db = (1 / objects_num) * np.sum(z - y_train)\n",
                "\n",
                "            W = np.diag(z * (1 - z))\n",
                "            hessian = (1 / objects_num) * (X_train.T @ W @ X_train)\n",
                "\n",
                "            hessian += 1e-6 * np.eye(hessian.shape[0])\n",
                "\n",
                "            try:\n",
                "                self.weights -= np.linalg.solve(hessian, dw)\n",
                "                self.bias -= db\n",
                "            except np.linalg.LinAlgError:\n",
                "                self.weights -= np.linalg.pinv(hessian) @ dw\n",
                "                self.bias -= db\n",
                "\n",
                "            if iteration % 100 == 0:\n",
                "                loss = self.log_loss(y_train, z)\n",
                "                self.losses.append(loss)\n",
                "\n",
                "    def predict(self, X_test, threshold=0.5):\n",
                "        t = np.dot(X_test, self.weights) + self.bias\n",
                "        z = self.sigmoid(t)\n",
                "        return (z > threshold).astype(int)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Исследование гиперпараметров"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"5. ИССЛЕДОВАНИЕ ГИПЕРПАРАМЕТРОВ\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "\n",
                "# Функция для вычисления метрик\n",
                "def calculate_metrics(y_true, y_pred):\n",
                "    accuracy = accuracy_score(y_true, y_pred)\n",
                "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
                "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
                "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
                "\n",
                "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1}\n",
                "\n",
                "\n",
                "# Параметры для исследования\n",
                "learning_rates = [0.001, 0.01, 0.1]  # Измененные значения\n",
                "iterations_list = [100, 500, 1000, 2000]  # Добавлены больше итераций\n",
                "\n",
                "print(\"МОДЕЛЬ 1: ГРАДИЕНТНЫЙ СПУСК\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "results_gd = []\n",
                "\n",
                "for lr in learning_rates:\n",
                "    for iters in iterations_list:\n",
                "        model = LogisticRegression()\n",
                "        model.gradient_descent(\n",
                "            X_train_scaled,\n",
                "            y_train.values,\n",
                "            iterations=iters,\n",
                "            learning_rate=lr,\n",
                "        )\n",
                "\n",
                "        y_pred = model.predict(X_test_scaled)\n",
                "\n",
                "        # Проверим распределение предсказаний\n",
                "        unique, counts = np.unique(y_pred, return_counts=True)\n",
                "        pred_dist = dict(zip(unique, counts))\n",
                "\n",
                "        metrics = calculate_metrics(y_test.values, y_pred)\n",
                "\n",
                "        results_gd.append(\n",
                "            {\n",
                "                \"method\": \"Градиентный спуск\",\n",
                "                \"learning_rate\": lr,\n",
                "                \"iterations\": iters,\n",
                "                \"pred_distribution\": pred_dist,\n",
                "                **metrics,\n",
                "            }\n",
                "        )\n",
                "\n",
                "        print(\n",
                "            f\"LR: {lr}, Iter: {iters}, Pred: {pred_dist}, \"\n",
                "            f\"Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_score']:.4f}\"\n",
                "        )\n",
                "\n",
                "print(\"\\nМОДЕЛЬ 2: ОПТИМИЗАЦИЯ НЬЮТОНА\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "results_nt = []\n",
                "\n",
                "for iters in iterations_list:\n",
                "    model = LogisticRegression()\n",
                "    model.newton_optimization(X_train_scaled, y_train.values, iterations=iters)\n",
                "\n",
                "    y_pred = model.predict(X_test_scaled)\n",
                "\n",
                "    unique, counts = np.unique(y_pred, return_counts=True)\n",
                "    pred_dist = dict(zip(unique, counts))\n",
                "\n",
                "    metrics = calculate_metrics(y_test.values, y_pred)\n",
                "\n",
                "    results_nt.append(\n",
                "        {\n",
                "            \"method\": \"Метод Ньютона\",\n",
                "            \"learning_rate\": \"-\",\n",
                "            \"iterations\": iters,\n",
                "            \"pred_distribution\": pred_dist,\n",
                "            **metrics,\n",
                "        }\n",
                "    )\n",
                "\n",
                "    print(\n",
                "        f\"Iter: {iters}, Pred: {pred_dist}, \"\n",
                "        f\"Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_score']:.4f}\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Лучшая модель и итоговые результаты"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"6. ЛУЧШАЯ МОДЕЛЬ И ИТОГОВЫЕ РЕЗУЛЬТАТЫ\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Находим лучшую модель по F1-score\n",
                "all_results = pd.DataFrame(results_gd + results_nt)\n",
                "best_result = all_results.loc[all_results[\"f1_score\"].idxmax()]\n",
                "\n",
                "print(\"ЛУЧШАЯ КОНФИГУРАЦИЯ:\")\n",
                "print(f\"Метод: {best_result['method']}\")\n",
                "print(f\"Learning rate: {best_result['learning_rate']}\")\n",
                "print(f\"Количество итераций: {best_result['iterations']}\")\n",
                "print(f\"Распределение предсказаний: {best_result['pred_distribution']}\")\n",
                "print()\n",
                "print(\"МЕТРИКИ ЛУЧШЕЙ МОДЕЛИ:\")\n",
                "print(f\"Accuracy: {best_result['accuracy']:.4f}\")\n",
                "print(f\"Precision: {best_result['precision']:.4f}\")\n",
                "print(f\"Recall: {best_result['recall']:.4f}\")\n",
                "print(f\"F1-Score: {best_result['f1_score']:.4f}\")\n",
                "print()\n",
                "\n",
                "# Обучение лучшей модели\n",
                "if best_result[\"method\"] == \"Градиентный спуск\":\n",
                "    best_model = LogisticRegression()\n",
                "    best_model.gradient_descent(\n",
                "        X_train_scaled,\n",
                "        y_train.values,\n",
                "        iterations=int(best_result[\"iterations\"]),\n",
                "        learning_rate=best_result[\"learning_rate\"],\n",
                "    )\n",
                "else:\n",
                "    best_model = LogisticRegression()\n",
                "    best_model.newton_optimization(\n",
                "        X_train_scaled.values, y_train.values, iterations=int(best_result[\"iterations\"])\n",
                "    )\n",
                "\n",
                "# Предсказания лучшей модели\n",
                "y_pred_best = best_model.predict(X_test_scaled)\n",
                "\n",
                "# Матрица ошибок\n",
                "\n",
                "cm = confusion_matrix(y_test.values, y_pred_best)\n",
                "\n",
                "plt.figure(figsize=(6, 5))\n",
                "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
                "plt.title(\"Матрица ошибок лучшей модели\")\n",
                "plt.ylabel(\"Истинные значения\")\n",
                "plt.xlabel(\"Предсказанные значения\")\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
